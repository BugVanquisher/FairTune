def eval_utility(model_id: str):
    """
    Placeholder: Evaluate task utility (accuracy, F1, etc).
    For now just return a dummy score.
    Later you can add real Q&A benchmarks.
    """
    return {
        "exact_match": 0.0,
        "f1": 0.0,
        "notes": f"Utility eval not implemented. Model: {model_id}"
    }